"""
Ø³ÙƒØ±Ø§Ø¨Ø± Ù…ØªÙ‚Ø¯Ù… Ù„Ù…Ù†ØµØ© Ø²Ø¯ Ù…Ø¹ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø°ÙƒÙŠ Ù„Ù„Ø£Ø³Ø¹Ø§Ø±
"""
import requests
from bs4 import BeautifulSoup
import time
import re
import logging
from typing import List, Dict, Optional
from urllib.parse import urljoin

from config import (
    BASE_URL, USER_AGENTS, REQUEST_TIMEOUT,
    RETRY_ATTEMPTS, RETRY_DELAY, PAGE_DELAY, MAX_PAGES
)

logger = logging.getLogger(__name__)


class ZidScraperException(Exception):
    """Ø§Ø³ØªØ«Ù†Ø§Ø¡ Ù…Ø®ØµØµ Ù„Ù„Ø³ÙƒØ±Ø§Ø¨Ø±"""
    pass


class ZidScraper:
    """Ø³ÙƒØ±Ø§Ø¨Ø± Ù…Ø­Ø³Ù‘Ù† Ù„Ù…Ù†ØµØ© Ø²Ø¯ Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø£Ø®Ø·Ø§Ø¡ Ù…ØªÙ‚Ø¯Ù…Ø©"""

    def __init__(self):
        self.session = requests.Session()
        self.products_found = 0
        self.pages_processed = 0
        self.errors_count = 0

    def _get_headers(self) -> Dict[str, str]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Headers Ù…Ø­Ø³Ù‘Ù†Ø©"""
        return {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ar,en-US;q=0.9,en;q=0.8',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }

    def _make_request(self, url: str, attempt: int = 1) -> Optional[requests.Response]:
        """Ø·Ù„Ø¨ HTTP Ù…Ø¹ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø©"""
        try:
            logger.info(f"ğŸ“¡ Ø·Ù„Ø¨ Ø§Ù„ØµÙØ­Ø©: {url} (Ù…Ø­Ø§ÙˆÙ„Ø© {attempt}/{RETRY_ATTEMPTS})")

            response = self.session.get(
                url,
                headers=self._get_headers(),
                timeout=REQUEST_TIMEOUT,
                allow_redirects=True
            )

            response.raise_for_status()
            return response

        except requests.exceptions.Timeout:
            logger.warning(f"â±ï¸ Ø§Ù†ØªÙ‡Øª Ù…Ù‡Ù„Ø© Ø§Ù„Ø·Ù„Ø¨ Ù„Ù„Ø±Ø§Ø¨Ø·: {url}")
        except requests.exceptions.ConnectionError:
            logger.warning(f"ğŸ”Œ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ø±Ø§Ø¨Ø·: {url}")
        except requests.exceptions.HTTPError as e:
            logger.warning(f"âŒ Ø®Ø·Ø£ HTTP {e.response.status_code}: {url}")
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹: {e}")

        # Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø©
        if attempt < RETRY_ATTEMPTS:
            wait_time = RETRY_DELAY * attempt
            logger.info(f"â³ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø¨Ø¹Ø¯ {wait_time} Ø«Ø§Ù†ÙŠØ©...")
            time.sleep(wait_time)
            return self._make_request(url, attempt + 1)

        self.errors_count += 1
        return None

    def _normalize_url(self, url: str) -> str:
        """ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø±Ø§Ø¨Ø· (Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù†Ø·Ø§Ù‚ Ø¥Ø°Ø§ ÙƒØ§Ù† Ù†Ø³Ø¨ÙŠØ§Ù‹)"""
        if url.startswith('http'):
            return url
        return urljoin(BASE_URL, url)

    def _extract_product_id(self, url: str) -> str:
        """
        Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø¹Ø±Ù Ø§Ù„Ù…Ù†ØªØ¬ Ù…Ù† Ø§Ù„Ø±Ø§Ø¨Ø· Ù…Ø¹ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù€ query parameters
        Ù…Ø«Ø§Ù„: /products/8972?variant=123 -> 8972
        """
        path = url.split('/')[-1]
        # Ø¥Ø²Ø§Ù„Ø© Ø£ÙŠ query parameters
        product_id = path.split('?')[0]
        return product_id

    def _extract_price(self, item: BeautifulSoup) -> str:
        """
        Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø³Ø¹Ø± Ø¨Ø°ÙƒØ§Ø¡ Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙÙˆØ§ØµÙ„ ÙˆØ§Ù„Ø£Ø±Ù‚Ø§Ù… Ø§Ù„ÙƒØ¨ÙŠØ±Ø©
        ğŸ”¥ Ù…Ø­Ø³Ù‘Ù† Ù„Ø¯Ø¹Ù… Ø§Ù„Ø£Ø³Ø¹Ø§Ø± Ù…Ø«Ù„: 1,200.00 Ùˆ 460.00
        """
        price_text = ""

        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø¹Ù†ØµØ± Ø§Ù„Ø³Ø¹Ø± Ø¨Ø·Ø±Ù‚ Ù…ØªØ¹Ø¯Ø¯Ø©
        price_selectors = [
            '.price .text-dark-1.fs-18px',
            '.text-dark-1.fs-18px',
            '.price',
            '[class*="price"]'
        ]

        for selector in price_selectors:
            price_elm = item.select_one(selector)
            if price_elm:
                price_text = price_elm.text.strip()
                break

        if not price_text:
            return "0.00"

        # ğŸ”¥ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø³Ø¹Ø±: Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ÙÙˆØ§ØµÙ„ ÙˆØ§Ù„Ø±Ù…ÙˆØ²
        # Ù…Ø«Ø§Ù„: "1,200.50 Ø±.Ø³" -> "1200.50"
        price_text = price_text.replace(',', '')  # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ÙÙˆØ§ØµÙ„
        price_text = price_text.replace('Ø±.Ø³', '').replace('SAR', '').strip()

        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±Ù‚Ù… Ø§Ù„Ø¹Ø´Ø±ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Regex
        match = re.search(r'(\d+\.?\d*)', price_text)

        if match:
            try:
                price_float = float(match.group(1))
                return f"{price_float:.2f}"
            except ValueError:
                logger.warning(f"âš ï¸ ÙØ´Ù„ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø³Ø¹Ø±: {price_text}")
                return "0.00"

        return "0.00"

    def _parse_product(self, item: BeautifulSoup) -> Optional[Dict]:
        """
        ØªØ­Ù„ÙŠÙ„ Ø¹Ù†ØµØ± Ù…Ù†ØªØ¬ ÙˆØ§Ø­Ø¯
        ğŸ”¥ Ø§Ù„Ù„ÙˆØ¬ÙŠÙƒ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ Ø¨Ø¯ÙˆÙ† ØªØ¹Ø¯ÙŠÙ„ - ÙÙ‚Ø· ØªØ­Ø³ÙŠÙ†Ø§Øª ÙÙŠ Ø§Ù„ÙƒÙˆØ¯
        """
        try:
            # 1. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±Ø§Ø¨Ø· ÙˆØ§Ù„Ø¹Ù†ÙˆØ§Ù†
            title_tag = item.select_one('.title a')
            if not title_tag:
                title_tag = item.select_one('a.product-card')

            if not title_tag:
                logger.debug("âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø±Ø§Ø¨Ø· Ø§Ù„Ù…Ù†ØªØ¬")
                return None

            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø§Ø³Ù… (Ù…Ù† Ø§Ù„Ù†Øµ Ø£Ùˆ Ù…Ù† attribute Ø§Ù„Ù€ title)
            name = title_tag.text.strip()
            if not name:
                name = title_tag.get('title', '').strip()

            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±Ø§Ø¨Ø·
            url = self._normalize_url(title_tag.get('href', ''))

            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù€ ID
            product_id = self._extract_product_id(url)

            if not name or not product_id:
                logger.debug("âš ï¸ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ù†ØªØ¬ Ù†Ø§Ù‚ØµØ©")
                return None

            # 2. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø³Ø¹Ø± (Ø¨Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ù…ÙØ­Ø³Ù‘Ù†Ø©)
            price = self._extract_price(item)

            # 3. ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø­Ø§Ù„Ø© (Ù…ØªÙˆÙØ± / Ù†Ø§ÙØ¯)
            status = "Available"

            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…Ø¤Ø´Ø±Ø§Øª Ù†ÙØ§Ø¯ Ø§Ù„ÙƒÙ…ÙŠØ©
            img_container = item.select_one('.img.position-relative')

            # Ø§Ù„Ù…Ø¤Ø´Ø± Ø§Ù„Ø£ÙˆÙ„: Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø±Ù…Ø§Ø¯ÙŠØ© (img-grayscale)
            has_grayscale = (
                img_container and
                'img-grayscale' in img_container.get('class', [])
            )

            # Ø§Ù„Ù…Ø¤Ø´Ø± Ø§Ù„Ø«Ø§Ù†ÙŠ: Ø²Ø± "ØºÙŠØ± Ù…ØªÙˆÙØ±"
            has_out_button = item.select_one('.btn-out-of-stock') is not None

            # Ø§Ù„Ù…Ø¤Ø´Ø± Ø§Ù„Ø«Ø§Ù„Ø«: Ù†Øµ "ØºÙŠØ± Ù…ØªÙˆÙØ±" ÙÙŠ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
            has_out_text = "ØºÙŠØ± Ù…ØªÙˆÙØ±" in item.text.lower()

            if has_grayscale or has_out_button or has_out_text:
                status = "Out of Stock"

            # Ø¨Ù†Ø§Ø¡ ÙƒØ§Ø¦Ù† Ø§Ù„Ù…Ù†ØªØ¬
            product = {
                'id': product_id,
                'name': name,
                'url': url,
                'price': price,
                'status': status
            }

            logger.debug(f"âœ… ØªÙ… ØªØ­Ù„ÙŠÙ„: {name[:50]}... - {status} - {price}")
            return product

        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù†ØªØ¬: {e}")
            return None

    def get_products(self, category_url: str) -> List[Dict]:
        """
        Ø³Ø­Ø¨ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª Ù…Ù† Ø§Ù„Ù‚Ø³Ù…
        ğŸ”¥ Ø§Ù„Ù„ÙˆØ¬ÙŠÙƒ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ Ù…Ø­ÙÙˆØ¸ Ø¨Ø§Ù„ÙƒØ§Ù…Ù„
        """
        all_products = []
        self.products_found = 0
        self.pages_processed = 0
        self.errors_count = 0

        logger.info(f"ğŸš€ Ø¨Ø¯Ø¡ ÙØ­Øµ Ø§Ù„Ù‚Ø³Ù…: {category_url}")

        for page in range(1, MAX_PAGES + 1):
            # Ø¨Ù†Ø§Ø¡ Ø±Ø§Ø¨Ø· Ø§Ù„ØµÙØ­Ø©
            url = f"{category_url}?page={page}"

            # Ø·Ù„Ø¨ Ø§Ù„ØµÙØ­Ø©
            response = self._make_request(url)
            if not response:
                logger.error(f"âŒ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙØ­Ø© {page}")
                break

            # ØªØ­Ù„ÙŠÙ„ HTML
            soup = BeautifulSoup(response.text, 'html.parser')

            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª
            product_items = soup.select('div.product')
            if not product_items:
                product_items = soup.select('.product-card')

            if not product_items:
                logger.info(f"ğŸ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ù†ØªØ¬Ø§Øª ÙÙŠ Ø§Ù„ØµÙØ­Ø© {page} - Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡")
                break

            logger.info(f"ğŸ“¦ ÙˆØ¬Ø¯Øª {len(product_items)} Ù…Ù†ØªØ¬ ÙÙŠ Ø§Ù„ØµÙØ­Ø© {page}")

            # ØªØ­Ù„ÙŠÙ„ ÙƒÙ„ Ù…Ù†ØªØ¬
            page_products = 0
            for item in product_items:
                product = self._parse_product(item)
                if product:
                    all_products.append(product)
                    page_products += 1

            self.products_found += page_products
            self.pages_processed += 1

            # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª Ø£Ù‚Ù„ Ù…Ù† 5ØŒ ØºØ§Ù„Ø¨Ø§Ù‹ Ù‡Ø°Ù‡ Ø¢Ø®Ø± ØµÙØ­Ø©
            if len(product_items) < 5:
                logger.info("ğŸ ØªÙ… Ø§Ù„ÙˆØµÙˆÙ„ Ù„Ø¢Ø®Ø± ØµÙØ­Ø©")
                break

            # Ø±Ø§Ø­Ø© Ø¨ÙŠÙ† Ø§Ù„ØµÙØ­Ø§Øª
            if page < MAX_PAGES:
                time.sleep(PAGE_DELAY)

        # ØªÙ‚Ø±ÙŠØ± Ù†Ù‡Ø§Ø¦ÙŠ
        logger.info(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘        ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø³ÙƒØ±Ø§Ø¨Ø± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ          â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ ğŸ“„ Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {self.pages_processed:>16} â•‘
â•‘ ğŸ“¦ Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª Ø§Ù„Ù…ÙƒØªØ´ÙØ©: {self.products_found:>15} â•‘
â•‘ âŒ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡: {self.errors_count:>25} â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        """)

        return all_products

    def test_connection(self, url: str) -> bool:
        """Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ù…ÙˆÙ‚Ø¹"""
        try:
            response = self._make_request(url)
            if response and response.status_code == 200:
                logger.info("âœ… Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ù†Ø§Ø¬Ø­")
                return True
            else:
                logger.error("âŒ ÙØ´Ù„ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ù…ÙˆÙ‚Ø¹")
                return False
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø§ØªØµØ§Ù„: {e}")
            return False
