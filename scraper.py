"""
Ø³ÙƒØ±Ø§Ø¨Ø± Ù…ØªÙ‚Ø¯Ù… Ù„Ù…Ù†ØµØ© Ø²Ø¯ Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø£Ø®Ø·Ø§Ø¡ Ø´Ø§Ù…Ù„Ø©
"""
import requests
from bs4 import BeautifulSoup
import time
import random
import logging
from typing import List, Dict, Optional
from urllib.parse import urljoin, urlparse

from config import (
    BASE_URL, USER_AGENTS, REQUEST_TIMEOUT,
    RETRY_ATTEMPTS, RETRY_DELAY, PAGE_DELAY, MAX_PAGES
)

logger = logging.getLogger(__name__)


class ZidScraperException(Exception):
    """Ø§Ø³ØªØ«Ù†Ø§Ø¡ Ù…Ø®ØµØµ Ù„Ù„Ø³ÙƒØ±Ø§Ø¨Ø±"""
    pass


class ZidScraper:
    """Ø³ÙƒØ±Ø§Ø¨Ø± Ù…Ø­Ø³Ù‘Ù† Ù„Ù…Ù†ØµØ© Ø²Ø¯"""

    def __init__(self):
        self.session = requests.Session()
        self.products_found = 0
        self.pages_processed = 0
        self.errors_count = 0

    def _get_headers(self) -> Dict[str, str]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù‡ÙŠØ¯Ø±Ø² Ø¹Ø´ÙˆØ§Ø¦ÙŠØ©"""
        return {
            'User-Agent': random.choice(USER_AGENTS),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ar,en-US;q=0.7,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }

    def _make_request(self, url: str, attempt: int = 1) -> Optional[requests.Response]:
        """Ø·Ù„Ø¨ HTTP Ù…Ø¹ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø©"""
        try:
            logger.info(f"ğŸ“¡ Ø·Ù„Ø¨ Ø§Ù„ØµÙØ­Ø©: {url} (Ù…Ø­Ø§ÙˆÙ„Ø© {attempt}/{RETRY_ATTEMPTS})")

            response = self.session.get(
                url,
                headers=self._get_headers(),
                timeout=REQUEST_TIMEOUT,
                allow_redirects=True
            )

            response.raise_for_status()
            return response

        except requests.exceptions.Timeout:
            logger.warning(f"â±ï¸ Ø§Ù†ØªÙ‡Øª Ù…Ù‡Ù„Ø© Ø§Ù„Ø·Ù„Ø¨ Ù„Ù„Ø±Ø§Ø¨Ø·: {url}")
        except requests.exceptions.ConnectionError:
            logger.warning(f"ğŸ”Œ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ø±Ø§Ø¨Ø·: {url}")
        except requests.exceptions.HTTPError as e:
            logger.warning(f"âŒ Ø®Ø·Ø£ HTTP {e.response.status_code}: {url}")
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹: {e}")

        # Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø©
        if attempt < RETRY_ATTEMPTS:
            wait_time = RETRY_DELAY * attempt
            logger.info(f"â³ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø¨Ø¹Ø¯ {wait_time} Ø«Ø§Ù†ÙŠØ©...")
            time.sleep(wait_time)
            return self._make_request(url, attempt + 1)

        self.errors_count += 1
        return None

    def _normalize_url(self, url: str) -> str:
        """ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø±Ø§Ø¨Ø· (Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù†Ø·Ø§Ù‚ Ø¥Ø°Ø§ ÙƒØ§Ù† Ù†Ø³Ø¨ÙŠØ§Ù‹)"""
        if url.startswith('http'):
            return url
        return urljoin(BASE_URL, url)

    def _extract_product_id(self, url: str) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø¹Ø±Ù Ø§Ù„Ù…Ù†ØªØ¬ Ù…Ù† Ø§Ù„Ø±Ø§Ø¨Ø·"""
        # Ù…Ø«Ø§Ù„: /products/8972 -> 8972
        # Ù…Ø«Ø§Ù„: /products/slug-name -> slug-name
        path = urlparse(url).path
        parts = path.strip('/').split('/')
        return parts[-1] if parts else url

    def _clean_text(self, text: str) -> str:
        """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ Ù…Ù† Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©"""
        if not text:
            return ""
        return ' '.join(text.split()).strip()

    def _extract_price(self, price_text: str) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø³Ø¹Ø± ÙˆØªÙ†Ø¸ÙŠÙÙ‡"""
        if not price_text:
            return "0.00"

        # Ø¥Ø²Ø§Ù„Ø© Ø±Ù…Ø² Ø§Ù„Ø±ÙŠØ§Ù„ ÙˆØ§Ù„Ù…Ø³Ø§ÙØ§Øª
        price = price_text.replace('Ø±.Ø³', '').replace('SAR', '').strip()
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø£Ø±Ù‚Ø§Ù… ÙÙ‚Ø·
        price = ''.join(c for c in price if c.isdigit() or c == '.')

        try:
            return f"{float(price):.2f}"
        except ValueError:
            return "0.00"

    def _parse_product(self, item: BeautifulSoup) -> Optional[Dict]:
        """ØªØ­Ù„ÙŠÙ„ Ø¹Ù†ØµØ± Ù…Ù†ØªØ¬ ÙˆØ§Ø­Ø¯"""
        try:
            # 1. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±Ø§Ø¨Ø· ÙˆØ§Ù„Ø¹Ù†ÙˆØ§Ù†
            link_tag = None

            # Ù…Ø­Ø§ÙˆÙ„Ø© 1: Ø§Ù„Ø¨Ø­Ø« ÙÙŠ div.title
            title_div = item.find('div', class_='title')
            if title_div:
                link_tag = title_div.find('a')

            # Ù…Ø­Ø§ÙˆÙ„Ø© 2: Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…Ø¨Ø§Ø´Ø± Ø¹Ù† Ø±Ø§Ø¨Ø· Ø§Ù„Ù…Ù†ØªØ¬
            if not link_tag:
                link_tag = item.find('a', href=lambda x: x and '/products/' in x)

            if not link_tag:
                logger.debug("âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø±Ø§Ø¨Ø· Ø§Ù„Ù…Ù†ØªØ¬")
                return None

            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
            name = self._clean_text(link_tag.get('title') or link_tag.text)
            url = self._normalize_url(link_tag.get('href', ''))
            product_id = self._extract_product_id(url)

            if not name or not product_id:
                logger.debug("âš ï¸ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ù†ØªØ¬ Ù†Ø§Ù‚ØµØ©")
                return None

            # 2. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø³Ø¹Ø±
            price = "0.00"
            price_div = item.find('div', class_='text-dark-1 fs-18px')
            if price_div:
                price = self._extract_price(price_div.text)

            # 3. ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø­Ø§Ù„Ø© (Ù…ØªÙˆÙØ± / Ù†Ø§ÙØ¯)
            status = "Available"

            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£Ø²Ø±Ø§Ø± "ØºÙŠØ± Ù…ØªÙˆÙØ±" Ø£Ùˆ "Out of Stock"
            out_of_stock_indicators = [
                item.find('a', class_='btn-out-of-stock'),
                item.find('button', class_='btn-out-of-stock'),
                item.find(text=lambda x: x and 'ØºÙŠØ± Ù…ØªÙˆÙØ±' in x.lower()),
                item.find(text=lambda x: x and 'out of stock' in x.lower()),
                item.find('div', class_='img-grayscale'),  # Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø±Ù…Ø§Ø¯ÙŠØ© ØªØ¯Ù„ Ø¹Ù„Ù‰ Ù†ÙØ§Ø¯
            ]

            if any(out_of_stock_indicators):
                status = "Out of Stock"

            product = {
                'id': product_id,
                'name': name,
                'url': url,
                'price': price,
                'status': status
            }

            logger.debug(f"âœ… ØªÙ… ØªØ­Ù„ÙŠÙ„: {name[:50]}... - {status}")
            return product

        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù†ØªØ¬: {e}")
            return None

    def get_products(self, category_url: str) -> List[Dict]:
        """Ø³Ø­Ø¨ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª Ù…Ù† Ø§Ù„Ù‚Ø³Ù…"""
        all_products = []
        self.products_found = 0
        self.pages_processed = 0
        self.errors_count = 0

        logger.info(f"ğŸš€ Ø¨Ø¯Ø¡ ÙØ­Øµ Ø§Ù„Ù‚Ø³Ù…: {category_url}")

        for page in range(1, MAX_PAGES + 1):
            # Ø¨Ù†Ø§Ø¡ Ø±Ø§Ø¨Ø· Ø§Ù„ØµÙØ­Ø©
            url = f"{category_url}?page={page}"

            # Ø·Ù„Ø¨ Ø§Ù„ØµÙØ­Ø©
            response = self._make_request(url)
            if not response:
                logger.error(f"âŒ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙØ­Ø© {page}")
                break

            # ØªØ­Ù„ÙŠÙ„ HTML
            soup = BeautifulSoup(response.text, 'html.parser')

            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª
            # ÙÙŠ Ù…Ù†ØµØ© Ø²Ø¯ØŒ Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª Ø¹Ø§Ø¯Ø© Ø¯Ø§Ø®Ù„ div.product
            product_items = soup.find_all('div', class_='product')

            if not product_items:
                logger.info(f"ğŸ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ù†ØªØ¬Ø§Øª ÙÙŠ Ø§Ù„ØµÙØ­Ø© {page} - Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡")
                break

            logger.info(f"ğŸ“¦ ÙˆØ¬Ø¯Øª {len(product_items)} Ù…Ù†ØªØ¬ ÙÙŠ Ø§Ù„ØµÙØ­Ø© {page}")

            # ØªØ­Ù„ÙŠÙ„ ÙƒÙ„ Ù…Ù†ØªØ¬
            page_products = 0
            for item in product_items:
                product = self._parse_product(item)
                if product:
                    all_products.append(product)
                    page_products += 1

            self.products_found += page_products
            self.pages_processed += 1

            # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª Ø£Ù‚Ù„ Ù…Ù† 10ØŒ ØºØ§Ù„Ø¨Ø§Ù‹ Ù‡Ø°Ù‡ Ø¢Ø®Ø± ØµÙØ­Ø©
            if len(product_items) < 10:
                logger.info("ğŸ ØªÙ… Ø§Ù„ÙˆØµÙˆÙ„ Ù„Ø¢Ø®Ø± ØµÙØ­Ø©")
                break

            # Ø±Ø§Ø­Ø© Ø¨ÙŠÙ† Ø§Ù„ØµÙØ­Ø§Øª
            if page < MAX_PAGES:
                time.sleep(PAGE_DELAY)

        # ØªÙ‚Ø±ÙŠØ± Ù†Ù‡Ø§Ø¦ÙŠ
        logger.info(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘        ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø³ÙƒØ±Ø§Ø¨Ø± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ          â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ ğŸ“„ Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {self.pages_processed:>16} â•‘
â•‘ ğŸ“¦ Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª Ø§Ù„Ù…ÙƒØªØ´ÙØ©: {self.products_found:>15} â•‘
â•‘ âŒ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡: {self.errors_count:>25} â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        """)

        return all_products

    def test_connection(self, url: str) -> bool:
        """Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ù…ÙˆÙ‚Ø¹"""
        try:
            response = self._make_request(url)
            if response and response.status_code == 200:
                logger.info("âœ… Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ù†Ø§Ø¬Ø­")
                return True
            else:
                logger.error("âŒ ÙØ´Ù„ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ù…ÙˆÙ‚Ø¹")
                return False
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø§ØªØµØ§Ù„: {e}")
            return False
